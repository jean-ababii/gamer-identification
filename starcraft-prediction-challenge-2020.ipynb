{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Notebook DataMining\n## Jean Ababii - Guilhem Baissus - Chanèle Jourdan"},{"metadata":{"id":"BCZwysPKCUCj"},"cell_type":"markdown","source":"# ----- 1. LOADING DATA"},{"metadata":{},"cell_type":"markdown","source":"The goal of this project is  to study how it is possible to determine who is playing given a behavioral trace (game events produced by the player) by designing a prediction model using machine learning techniques. We'll try to solve this problem with the video game StarCraft 2. \n\nWe'll work with these two datasets :\n\n* TRAIN - the training set: labelled behavioral traces\n* TEST - the test set: unlabelled behavioral traces (you need to predict the player)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"Thr7yLaPCUCk","executionInfo":{"status":"ok","timestamp":1605999987744,"user_tz":-60,"elapsed":700,"user":{"displayName":"Jean","photoUrl":"","userId":"01045939193516150342"}},"outputId":"aa1193bf-ef15-4b6f-d5e4-b31f8a76fc38"},"cell_type":"code","source":"# IMPORTS\n\nimport numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\n\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"rsnY__ToCUCm","executionInfo":{"status":"error","timestamp":1605999989757,"user_tz":-60,"elapsed":664,"user":{"displayName":"Jean","photoUrl":"","userId":"01045939193516150342"}},"outputId":"afb25884-02a5-462c-acce-c29c8f5c3a33"},"cell_type":"code","source":"# GET TRAIN AND TEST DATASETS\n\npath_train_dataset = os.path.join(dirname, \"TRAIN.CSV\")\npath_test_dataset = os.path.join(dirname, \"TEST.CSV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9EW4t7mcCUCm"},"cell_type":"code","source":"# VARIABLES INFLUENCING THE RESULTS\n\ntime_limit = 120\nminimum_keys_pressed = 20\n#number_of_features = 0","execution_count":null,"outputs":[]},{"metadata":{"id":"mgOu7FVJCUCm"},"cell_type":"markdown","source":"# ----- 2. DATA VISUALISATION AND PRE-PROCESSING"},{"metadata":{},"cell_type":"markdown","source":"For our pre-preocessing we'll load the 2 datasets in dictionnaries and realise various operations on our data. "},{"metadata":{"trusted":true,"id":"W6ccSkEDCUCm","outputId":"55c8580f-14f9-4621-ea92-53537004801f"},"cell_type":"code","source":"#Get number of lines of the datasets train and test\n\ndef get_number_of_lines_in_csv(path): \n    file = open(path)\n    reader = csv.reader(file)\n    nb_lines = len(list(reader))\n    return nb_lines\n    \nnb_lines_train = get_number_of_lines_in_csv(path_train_dataset)\nprint(\"Number of lines in train dataset : \", nb_lines_train)\nnb_lines_test = get_number_of_lines_in_csv(path_test_dataset) \nprint(\"Number of lines in test dataset : \",nb_lines_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qFdubgF9CUCm","outputId":"7753ce93-d66f-483f-eae4-f68d16fad999","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#Get train's data and test's data into dictionnaries\n\n#gets array of the different elements of the row before time_limit. If time_limit = 0 means no time limit.\ndef get_row_elements(row_str, time_limit):\n    if int(time_limit)== 0:\n        elements = row_str.split(',')\n        return elements\n    else:      \n        elements = row_str.split(',') \n        for index, element in enumerate(elements):\n            if \"t\" in element[0] and int(element[1:]) >= int(time_limit):\n                del elements[index:]\n                return elements\n        return elements\n\n#saves data in a dictionnary\ndef fill_dic (dic, row_number, row_elements):\n    dic[row_number]= row_elements\n\n#creates dictionnary with the row number as a key and an array of all the elements of the specific line\ndef create_dictionnary(path, nb_lines, time_limit):    \n    dic = {}\n\n    df = pd.read_csv(path, sep='delimiter', header=None)\n    columns = df.columns\n    rows = df[columns[0]]\n    for index in range(0,nb_lines):\n        row_str = rows[index]\n        row_elements = get_row_elements(row_str, time_limit)\n        fill_dic(dic, index, row_elements)\n    return dic\n\n#time_limit = 30\n\ntrain_rows = create_dictionnary(path_train_dataset, nb_lines_train, time_limit)\n#print(train_rows)\n\ntest_rows = create_dictionnary(path_test_dataset, nb_lines_test, time_limit)\n#print(test_rows)","execution_count":null,"outputs":[]},{"metadata":{"id":"M7U30715CUCn"},"cell_type":"markdown","source":"# 2.1 Get features"},{"metadata":{},"cell_type":"markdown","source":"We'll get and create various features from our dataset :\n\n* The list of all the keys pressed to get the number of times a player used each keys during a game\n* The race played (3 types of race, so we create 3 features with a value set to 1 or 0 according the race of each game)\n* A string which identifies the player\n* The total of the number of keys pressed\n* Time features : the number of keys pressed in every time interval of 5 seconds"},{"metadata":{"trusted":true,"id":"U7g8XYrBCUCn","outputId":"b8253d2d-f77a-4f29-c4b0-d30ffa422336"},"cell_type":"code","source":"#Get columns of the datasets\n\n#conditions allowing the extraction of elements to make the features\ndef extraction_conditions(element):\n    return not ('t1' in element or 't2' in element or 't3' in element or 't4' in element or 't5' in element or 't6' in element or 't7' in element or 't8' in element or 't9' in element or element in dataset_columns or \"http\" in element or \"Protoss\" in element or \"Terran\" in element or \"Zerg\" in element)\n\n#Creates the columns of the dataset from the values extracted per row\ndef get_dataset_columns(dic, nb_lines):\n    for index in range(0, nb_lines):\n        row = train_rows[index]\n        for element in row:\n            if extraction_conditions(element) :\n                dataset_columns.append(element)\n                \n        \n#I added 4 more colums : profile and the species : \"profile\" is the class we want to predict and species the type of character that the player chose that can either be \"Protoss\" or \"Terran\" or \"Zerg\". It will allow a manual hot encoding\ndataset_columns = ['player_profile', 'Protoss', 'Terran', 'Zerg'] \n\nget_dataset_columns(train_rows, nb_lines_train)\nprint(dataset_columns)\n\n#number of columns\nprint('----------------------')\nprint(\"Number of columns : {}\".format(len(dataset_columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"oqu6WrsYCUCo"},"cell_type":"code","source":"#Adding time features columns\n#We are going to try do add features linked to time to see if the results change positively\n\n#Every 5 seconds until time limit, create a column where the number of keys pressed is going to be recorded\ndef add_5seconds_columns(time_limit, columns):\n    time = 0\n    for i in range(0,int(time_limit/5)):\n        time +=5\n        columns.append(\"t{} to t{}\".format(time - 5, time))\n\ndef add_total_keys_pressed(columns):\n    columns.append(\"total_keys_pressed\")\n    \n#add_5seconds_columns(time_limit, dataset_columns)\n#add_total_keys_pressed(dataset_columns)\n    \nprint(dataset_columns)\n\n#number of columns\nprint('----------------------')\nprint(\"Number of columns : {}\".format(len(dataset_columns)))    ","execution_count":null,"outputs":[]},{"metadata":{"id":"4etaOvd7CUCo"},"cell_type":"markdown","source":"# 2.2 Cleaning data"},{"metadata":{},"cell_type":"markdown","source":"* We'll plot the number of keys pressed in every game and see if the distribution is homogeneous, and adapt the data consequently\n"},{"metadata":{"trusted":true,"id":"iMer0QT2CUCo","outputId":"53376116-d908-4ce4-dc85-baaae6e31dcb","_kg_hide-output":false},"cell_type":"code","source":"#Data distribution for the time limit chosen \n    \ndef get_key_frequence_per_game(dic, nb_lines):\n    nb_key = []\n    for index in range(0, nb_lines):\n        row = dic[index]\n        size_row = len(row)\n        i = size_row -1\n        while row[i][0]!=\"t\" and i>0:\n            i-=1\n        if i!=0:\n            time_value = row[i][1:]\n            #              size - number of t present - 2 for the first two columns (id and specie chosen)\n            nb_key.append(size_row - int(time_value)/5 - 2)\n        else:\n            #In case there is no time stamp on the line\n            nb_key.append(size_row - 2)\n            #if size_row - 2 ==0:\n                #print(\"no value for a line\")\n        \n    return nb_key\n\ndef plot_data_visualisation(dic, nb_lines):\n    nb_key = get_key_frequence_per_game(dic, nb_lines)\n    plt.hist(nb_key, bins=40)  \n    plt.grid(axis='y', alpha=0.75)\n    plt.xlabel('Number of keys pressed')\n    plt.ylabel('Number of games')\n    plt.title(\"Distribution of the number of keys pressed in {} secondes\".format(time_limit))\n    plt.show()\n\nplt.figure(figsize = (20,10))\nplot_data_visualisation(train_rows, nb_lines_train)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TfT3Jog-CUCp"},"cell_type":"markdown","source":"We see thate there's a big varibility in the game time and the number of keys pressed. \nTherefore, it's necessary to adjust our data by :\n\n- Removing empty games or with a game time really short to not biases our data processing. \n- Normalising data"},{"metadata":{"trusted":true,"id":"g-uNbo9tCUCp","outputId":"b1e86c48-f732-4e1f-9a3a-c54823a3192b"},"cell_type":"code","source":"#get lines having less than a minimum number of keys pressed\ndef not_enough_values_lines_indexes(dic, nb_lines, minimum_keys_pressed):\n    lines_indexes = []\n    for index in range(0, nb_lines):\n        row = dic[index]\n        size_row = len(row)    \n        i = size_row -1\n        while row[i][0]!=\"t\" and i>0:\n            i-=1\n        if i!=0:\n            time_value = row[i][1:]\n            #              size - number of t present - 2 for the first two columns (id and specie chosen)\n            size = size_row - int(time_value)/5 - 2\n        else:\n            #In case there is no time stamp on the line\n            size = size_row - 2\n        if size <minimum_keys_pressed:\n            lines_indexes.append(index)\n\n    return lines_indexes\n\nlines_no_value_indexes = []\n#minimum_keys_pressed = 20\n\nlines_no_value_indexes = not_enough_values_lines_indexes(train_rows, nb_lines_train, minimum_keys_pressed)\nprint(\"{} lines need to be removed because they have less than {} values\".format(len(lines_no_value_indexes), minimum_keys_pressed))\nprint(lines_no_value_indexes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get here a liste which contains the indexes of all the lines having less than 20 values. This list we'll then be used for the creation of the dataset.  "},{"metadata":{"id":"yiq1eqVLCUCp"},"cell_type":"markdown","source":"# 2.3 Solved unbalanced class"},{"metadata":{},"cell_type":"markdown","source":"We'll plot here the number of games of each players to see if the distribution is homogeneous, and adapt the data consequently.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"with open(path_train_dataset, 'r') as temp_f:\n    # get No of columns in each line\n    col_count = [ len(l.split(\",\")) for l in temp_f.readlines() ]\n    \ncolumn_names_train = ['Player','Race'] + [i for i in range(0, max(col_count)-2)]\ndf = pd.read_csv(path_train_dataset, header=None, delimiter=\",\", names=column_names_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9-JyQlzyCUCp","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# Plot the number of games by player\n\ndf_by_player = df.groupby(['Player']).count().sort_values(by=['Race'])['Race']\n\navg = df_by_player.mean()\nprint('Average of number of games by player: ', avg)\n\nbar_chart = df_by_player.plot.bar(x='Player', y='Race', rot=0, figsize = (20,10))\nbar_chart.hlines(avg, -.5,200.5, linestyles='dashed')\nbar_chart.annotate('average',(10,avg+1))\nbar_chart.axes.get_xaxis().set_visible(False)\nbar_chart.set_title(\"Distribution of the number of games by player\", fontsize=20)\nbar_chart\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5PUfE_68CUCp"},"cell_type":"markdown","source":"We see that the number of games by players is really unbalances, which can be problematic especially for cross validation. There for we have to process data by one of these two possiblities :\n\n- Duplicate data for the players having a number of games really low. \n- Remove data for players having a big number of games (but it's maybe less relevant because it implies loosing information)\n\nSo we decide to increase data for player having a game's number under the average (which is around 15), by duplicating one or two times depending of the game's number. "},{"metadata":{"trusted":true,"id":"Fw3517jwCUCq"},"cell_type":"code","source":"# Get list of players with less than 15 games in dataframe\n\ndf_mod = df.groupby(['Player']).count()\ndf_0to5 = df_mod[(df_mod['Race']<6)]\ndf_0to15 = df_mod[(df_mod['Race']<15)]\ndf_6to15 = df_0to15[(df_0to15['Race']>5)]\nplayer_6to15 = df_6to15.index.tolist()\nplayer_0to5 = df_0to5.index.tolist()\n\nprint(\"Number of players with 0 to 5 games : \", len(player_0to5))\nprint(\"Number of players with 6 to 15 games : \", len(player_6to15))\nprint(\"--------------------------------------\")\n\n# Duplicate data for this players (1 ot 2 times according their number of games) in dataframe\n\nrows_to_add=[]\nrows_indexes_add_once=[]\nrows_indexes_add_twice=[]\nfor indexRow in range(nb_lines_train):\n    row=df.values[indexRow]\n    if(row[0] in player_6to15):\n        rows_indexes_add_once.append(indexRow)\n        rows_to_add.append(row)\n    elif(row[0] in player_0to5):\n        rows_indexes_add_twice.append(indexRow)\n        rows_to_add.append(row)\n        rows_to_add.append(row)\n        \ndf_with_duplication=pd.concat([df,pd.DataFrame(rows_to_add, columns=df.columns)], ignore_index=True)\n\n# Add duplicates in dictionnary \n\nprint(\"Length of the dictionnary before duplication :\",nb_lines_train)\n\ncounter=0;\nfor indexRow in range(len(rows_indexes_add_once)):\n    train_rows[3052+counter]=train_rows[rows_indexes_add_once[indexRow]]\n    counter=counter+1\nfor indexRow in range(len(rows_indexes_add_twice)):\n    train_rows[3052+counter]=train_rows[rows_indexes_add_once[indexRow]]\n    train_rows[3052+counter+1]=train_rows[rows_indexes_add_once[indexRow]]\n    counter=counter+2\n    \nnb_lines_train=len(train_rows)\nprint(\"Number of games to duplicate :\",len(rows_indexes_add_once)+2*len(rows_indexes_add_twice))\n\nprint(\"Length of the dictionnary after duplication :\",nb_lines_train)\n\n\n# Plot the number of games by player with new distribution\n\ndf_with_duplication_by_player = df_with_duplication.groupby(['Player']).count().sort_values(by=['Race'])['Race']\nbar_chart = df_with_duplication_by_player.plot.bar(x='Player', y='Race', rot=0, figsize = (20,8))\nbar_chart.hlines(15.26, -.5,200.5, linestyles='dashed')\nbar_chart.annotate('old average',(10,avg+1))\nbar_chart.axes.get_xaxis().set_visible(False)\nbar_chart.set_title(\"Distribution of the number of games by player after duplication\", fontsize=20)\n\nbar_chart\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ygLCDqE_CUCq"},"cell_type":"markdown","source":"# 2.4 Extract features & Create dataframe"},{"metadata":{},"cell_type":"markdown","source":"Finally, we create the dataframe from all the modification we've done before, and with the list of features we described. "},{"metadata":{"trusted":true,"id":"4xC-dc0zCUCq"},"cell_type":"code","source":"#Organized data into columns (frequencies of keys per line + species and profile + time features) and create a dataframe for each dataset\n\n#creates empty dataset\ndef create_empty_dataset(columns):\n    dataset = {}\n    for column in columns:\n        dataset[column] = []\n    return dataset\n\n#Gets the number of times one key was clicked\ndef get_number_clicked_key(row, key_name):\n    total_number = 0\n    for element in row:\n        if element == key_name:\n            total_number +=1\n    return total_number\n\n#returns the profile of the player\ndef get_profile(row):\n    if \"http\" in row[0]:\n        return row[0]\n    else:\n        return \"Unknown\"\n    \n\n#returns the specie chosen on this line\ndef get_specie(dataset, row):    \n    column_number = 0\n    #Looking for the column containing the profile\n    for index,element in enumerate(row):\n        if element == \"Protoss\" or  element == \"Terran\" or element ==\"Zerg\":\n            column_number = index\n            break\n        \n    if row[column_number]==\"Protoss\":\n        dataset[\"Protoss\"].append(1.0) \n        dataset[\"Terran\"].append(0.0)\n        dataset[\"Zerg\"].append(0.0)\n    elif row[column_number] == \"Terran\":\n        dataset[\"Protoss\"].append(0.0) \n        dataset[\"Terran\"].append(1.0)\n        dataset[\"Zerg\"].append(0.0)\n    elif row[column_number] == \"Zerg\":\n        dataset[\"Protoss\"].append(0.0) \n        dataset[\"Terran\"].append(0.0)\n        dataset[\"Zerg\"].append(1.0)\n    else:\n        print(\"Profile not detected\")\n\n#count the number of keys pressed in a 5 seconds interval until time limit\ndef get_key_number_5seconds(dataset, row, time_limit):  \n    number_elements_in_frame = 0\n    time = 5\n    for index, element in enumerate(row):\n        if \"t{}\".format(time) in element or index == len(row)-1:\n            #I have to pop because a 0 appears for a unknown reason\n            dataset[\"t{} to t{}\".format(time -5, time)].pop()\n            dataset[\"t{} to t{}\".format(time -5, time)].append(number_elements_in_frame)\n            number_elements_in_frame = 0\n            time +=5\n        #does not take in account the elements linked to the player profile or the specie chosen\n        if \"http\" in element or element == \"Zerg\" or element == \"Protoss\" or element == \"Terran\":\n            number_elements_in_frame -=1\n        number_elements_in_frame +=1\n        \n#count total number of keys pressed between 0 and the time limit\ndef get_total_keys_pressed(dataset, row):\n    number_elements_in_row = 0\n    for index, element in enumerate(row):\n        number_elements_in_row +=1\n        if \"http\" in element or element == \"Zerg\" or element == \"Protoss\" or element == \"Terran\" or \"t1\" in element or \"t2\" in element or \"t3\" in element or \"t4\" in element or \"t5\" in element or \"t6\" in element or \"t7\" in element or \"t8\" in element or \"t9\" in element:\n            number_elements_in_row -=1\n    dataset[\"total_keys_pressed\"].pop()\n    dataset[\"total_keys_pressed\"].append(number_elements_in_row)\n\n                \n\n#create final dataset with the columns filled\ndef create_dataset(nb_lines, rows, time_limit, dataset_type = \"train\"):\n    dataset = create_empty_dataset(dataset_columns)\n\n    #For each line, the columns are filled\n    for index in range(0,nb_lines):\n        #Do not take in account the lines with not enough values\n        if index not in lines_no_value_indexes or dataset_type ==\"test\":\n            #Prevent \"species\" and profiles columns to be called\n            for column in dataset_columns[4:]:\n                row = rows[index]\n                total_number = get_number_clicked_key(row, column)\n                dataset[column].append(total_number)\n            dataset['player_profile'].append(get_profile(rows[index]))\n            get_specie(dataset, rows[index])\n            #get_key_number_5seconds(dataset, rows[index], time_limit)\n            #get_total_keys_pressed(dataset, rows[index])\n    return dataset\n            \ntrain_dataset = create_dataset(nb_lines_train, train_rows, time_limit, \"train\")\n#print(train_dataset)\ntest_dataset =  create_dataset(nb_lines_test, test_rows, time_limit, \"test\")\n\n#Create dataframes\ntrain_df = pd.DataFrame(train_dataset,columns=dataset_columns)\ntest_df = pd.DataFrame(test_dataset,columns=dataset_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2TAl5xv4CUCq"},"cell_type":"code","source":"#Create csv files from the dataframes\n#train_df.to_csv(\"train_df.csv\", index = False)\n#test_df.to_csv(\"test_df.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"ApkWLRnLCUCq"},"cell_type":"markdown","source":"# ----- 3.DIMENSIONALITY REDUCTION"},{"metadata":{"id":"cZz27OOVCUCq"},"cell_type":"markdown","source":"Dimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data\n\nWhy doing that?\n* Reducing training time\n* Using less memory \n* Improves Accuracy (less misleading data means modeling accuracy improves) : even if it's not the main advantage\n* Reducing risk of overfitting (Less redundant data means less opportunity to make decisions based on noise)\n* Avoiding the “curse of dimensionality” (problem that happens when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. For example :  in high dimensional data, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient).\n\nWe can define two types of dimensionality reduction : feature selection and feature extraction\n"},{"metadata":{"id":"ckglfT9xCUCq"},"cell_type":"markdown","source":"# 3.1 Feature selection"},{"metadata":{"id":"hf6twxI5CUCq"},"cell_type":"markdown","source":"Some features in data can be redundant or irrelevant, and can thus be removed without incurring much loss of information. So we try to find a subset of the input variables => reduce dimensionality by removing some features.\n\nThere are 3 different methods for feature selection :\n\n**1.FILTER METHOD**\n\nFilter type methods select variables regardless of the model -> suppress the least interesting variables. The inconvenient is that it tends to select redundant variables when they do not consider the relationships between variables.\n\n\n**2.WRAPPER METHOD**\n\nThis method consist in evaluating subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. (but in comparison to the first method, it can increase overfitting risk + significant computation time)\n\n**3.EMBEDDED METHOD**\n\nThis method tries to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously\n"},{"metadata":{"id":"IbgyXD-HCUCq"},"cell_type":"markdown","source":"***FIRST EXAMPLE: using feature importance (filter method)***"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(columns=['player_profile'])\ny_train = train_df['player_profile']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExtraTreesClassifier()\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfig = feat_importances.nlargest(36).plot(kind='barh', figsize=(20,10), title = \"Feature importance\", xlabel =\"key\", ylabel = \"score\")\nfig.set_title(\"Feature importance\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new dataframe only with the selected features\ntrain_fi19_df = train_df.drop(columns =[\"player_profile\", \"hotkey11\", \"hotkey51\", \"hotkey31\", \"hotkey71\", \"hotkey21\",\n                                      \"hotkey81\", \"hotkey41\", \"hotkey61\", \"hotkey72\", \"hotkey82\", \"hotkey91\", \"hotkey92\",\n                                      \"hotkey01\", \"Terran\", \"Zerg\", \"Protoss\", \"hotkey02\"])\ntest_fi19_df  = test_df.drop(columns =[\"player_profile\", \"hotkey11\", \"hotkey51\", \"hotkey31\", \"hotkey71\", \"hotkey21\",\n                                      \"hotkey81\", \"hotkey41\", \"hotkey61\", \"hotkey72\", \"hotkey82\", \"hotkey91\", \"hotkey92\",\n                                      \"hotkey01\", \"Terran\", \"Zerg\", \"Protoss\", \"hotkey02\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new dataframe only with the selected features\ntrain_fi11_df = train_df.drop(columns =[\"player_profile\", \"hotkey11\", \"hotkey51\", \"hotkey31\", \"hotkey71\", \"hotkey21\", \"hotkey81\", \"hotkey41\", \"hotkey61\", \"hotkey72\", \n                                        \"hotkey82\", \"hotkey91\", \"hotkey92\", \"hotkey01\", \"Terran\", \"Zerg\", \"Protoss\", \"hotkey02\", \"Base\", \"hotkey80\", \"SingleMineral\",\n                                       \"hotkey62\", \"hotkey70\", \"hotkey90\", \"hotkey60\", \"hotkey00\"])\ntest_fi11_df  = test_df.drop(columns =[\"player_profile\", \"hotkey11\", \"hotkey51\", \"hotkey31\", \"hotkey71\", \"hotkey21\", \"hotkey81\", \"hotkey41\", \"hotkey61\", \"hotkey72\", \n                                        \"hotkey82\", \"hotkey91\", \"hotkey92\", \"hotkey01\", \"Terran\", \"Zerg\", \"Protoss\", \"hotkey02\", \"Base\", \"hotkey80\", \"SingleMineral\",\n                                       \"hotkey62\", \"hotkey70\", \"hotkey90\", \"hotkey60\", \"hotkey00\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"XkE-NqYDCUCq"},"cell_type":"markdown","source":"***SECOND EXAMPLE: UNIVARIATE SELECTION***\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract all the features\nnb_of_features = 36\nbestfeatures = SelectKBest(score_func=chi2, k=nb_of_features)\nfit = bestfeatures.fit(X_train,y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['action','score']  #naming the dataframe columns\n\n#show best features\nfig = featureScores.nlargest(36,'score').plot(x='action', kind='barh', figsize=(20,10), xlabel =\"key\", ylabel = \"score\")\nfig.set_title(\"Univariate selection of the best attributes\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new dataframe only with the selected features\ntrain_univariate18_df = train_df.drop(columns =[\"player_profile\", \"Protoss\", \"Zerg\", \"hotkey71\", \"hotkey80\", \"hotkey51\", \"Terran\", \"hotkey70\", \"hotkey61\", \"hotkey81\",\n                                                \"hotkey50\", \"hotkey90\", \"hotkey00\", \"hotkey91\", \"hotkey01\", \"hotkey11\", \"hotkey41\", \"hotkey21\", \"hotkey31\"])\ntest_univariate18_df  = test_df.drop(columns =[\"player_profile\", \"Protoss\", \"Zerg\", \"hotkey71\", \"hotkey80\", \"hotkey51\", \"Terran\", \"hotkey70\", \"hotkey61\", \"hotkey81\",\n                                                \"hotkey50\", \"hotkey90\", \"hotkey00\", \"hotkey91\", \"hotkey01\", \"hotkey11\", \"hotkey41\", \"hotkey21\", \"hotkey31\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new dataframe only with the selected features\ntrain_univariate13_df = train_df.drop(columns =[\"player_profile\", \"Protoss\", \"Zerg\", \"hotkey71\", \"hotkey80\", \"hotkey51\",\"Terran\", \"hotkey70\", \"hotkey61\", \"hotkey81\",\n                                                \"hotkey50\", \"hotkey90\", \"hotkey00\", \"hotkey91\", \"hotkey01\", \"hotkey11\", \"hotkey41\", \"hotkey21\", \"hotkey31\", \"hotkey40\",\n                                                \"hotkey60\", \"hotkey10\", \"hotkey20\", \"hotkey30\"])\ntest_univariate13_df  = test_df.drop(columns =[\"player_profile\", \"Protoss\", \"Zerg\", \"hotkey71\", \"hotkey80\", \"hotkey51\",\"Terran\", \"hotkey70\", \"hotkey61\", \"hotkey81\",\n                                                \"hotkey50\", \"hotkey90\", \"hotkey00\", \"hotkey91\", \"hotkey01\", \"hotkey11\", \"hotkey41\", \"hotkey21\", \"hotkey31\", \"hotkey40\",\n                                                \"hotkey60\", \"hotkey10\", \"hotkey20\", \"hotkey30\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"eh4yTdz-CUCq"},"cell_type":"markdown","source":"***THIRD EXAMPLE: using low variance (filter method)***"},{"metadata":{"trusted":true,"id":"igeBDmE_CUCq","outputId":"02da4e19-a982-46bc-b067-90379d56d0a1"},"cell_type":"code","source":"#Code avec VarianceThreshold de SKLearn\nfrom sklearn.feature_selection import VarianceThreshold\n\nfs = VarianceThreshold(5)\nxtrain_var = fs.fit_transform(np.array(train_df.drop(\"player_profile\", 1)))\nfs.get_support()\ntrain_var_df = train_df[train_df.columns[fs.get_support(indices=True)]]\ntest_var_df  = test_df[test_df.columns[fs.get_support(indices=True)]]\n\nprint(train_var_df.columns)\nprint(\"Number of features selected :\",len(train_var_df.columns))","execution_count":null,"outputs":[]},{"metadata":{"id":"6sihSu04CUCr"},"cell_type":"markdown","source":"# 3.2 Feature extraction"},{"metadata":{},"cell_type":"markdown","source":"We start from an initial set of measured data and builds derived features intended to be informative and non-redundant => constructing these derived features with combinations of the initial variables : facilitating the subsequent learning and generalization steps.\n\nThere are many ways to do feature extraction.\n\n"},{"metadata":{"id":"iusqAkEuCUCr"},"cell_type":"markdown","source":"\n\n***FOURTH EXAMPLE: using PCA (principal component analysis)***"},{"metadata":{"trusted":true,"id":"XDmlSwciCUCr"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalization\nX_train = StandardScaler().fit_transform(X_train)\n\n#PCA\npca = PCA(n_components=nb_of_features) #nb_of_features = 36\nprincipalComponents = pca.fit_transform(X_train)\nprincipalDf = pd.DataFrame(data = principalComponents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot explained variance\nplt.figure(figsize=(18,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.grid()\nplt.title(\"Cumulative explained variance in function of the number of components\",fontsize=20)\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We choose first 20 attributes (around 20% of the variance introduced) "},{"metadata":{"id":"AJ80PUOACUCr"},"cell_type":"markdown","source":"# ----- 4.MODEL CONSTRUCTION AND PREDICTION"},{"metadata":{"trusted":true,"id":"E0L3Dl9cCUCr"},"cell_type":"code","source":"#Scale X\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale(X):\n    X = StandardScaler().fit_transform(X)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"JcDsoDfhCUCr"},"cell_type":"code","source":"#Evaluate the models\ndef evaluate(dataset, model, n_splits):\n\n    X = np.array(train_df.drop(\"player_profile\", 1))\n    X = scale(X)\n    y = np.array(train_df[\"player_profile\"])\n\n    le = preprocessing.LabelEncoder()\n    le.fit(y)\n    y = le.transform(y)\n\n    #array\n    scores_accuracy = []\n\n    kf = KFold(n_splits=n_splits)\n    nb_folds_processed = 0\n    for train_index, test_index in kf.split(X, y):\n        nb_folds_processed +=1\n        print(\" {} folds processed\".format(nb_folds_processed))\n        x_train, x_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n\n        scores_accuracy.append(accuracy_score(y_test, y_pred))\n\n    return np.mean(scores_accuracy)\n\nnb_splits = 4\n\nmodel = DecisionTreeClassifier(criterion=\"entropy\")\nprint(evaluate(train_df,model, nb_splits ))\n\nmodel = RandomForestClassifier(n_estimators=100,bootstrap = True, max_features = 'sqrt')\n#print(evaluate(train_df,model, nb_splits ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"idCz9SzwCUCr"},"cell_type":"code","source":"#Train decision tree model with the train dataset \n\ndef train_decision_tree(df):\n    #variables\n    test_size = 0.1\n    criterion = \"entropy\"\n\n    X_train = np.array(df.drop(\"player_profile\", 1))\n    y_train = np.array(df[\"player_profile\"])\n\n    model = DecisionTreeClassifier(criterion=criterion)\n    model = model.fit(X_train, y_train)\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"VDC7oFRCCUCr"},"cell_type":"code","source":"#Train random forest model\n\ndef train_random_forest(df):\n\n    X_train = np.array(df.drop(\"player_profile\", 1))\n    y_train = np.array(df[\"player_profile\"])\n\n    model = RandomForestClassifier(n_estimators=100, \n                                   bootstrap = True,\n                                   max_features = 'sqrt')\n\n    model = model.fit(X_train, y_train)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"3aqIZGbeCUCr"},"cell_type":"markdown","source":"# ----- 5. Results and conclusion"},{"metadata":{"trusted":true,"id":"ly8GtlXBCUCr"},"cell_type":"code","source":"def get_predictions(model):\n    X_test = np.array(test_df.drop(\"player_profile\", 1))\n    y_test = np.array(test_df[\"player_profile\"])\n\n    y_pred =  model.predict(X_test)\n    return y_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"NoWDkLDICUCr"},"cell_type":"code","source":"#We create a new csv containing the predictions \ndef create_csv(y_pred, name=\"results.csv\"):\n    rowid = []\n    for i in range(1, nb_lines_test+1):\n        rowid.append(i)\n\n    data = {\n            \"RowId\" : rowid,\n            \"prediction\" : y_pred,\n        }\n\n    df = pd.DataFrame()\n    df = pd.DataFrame(data,columns=list(data.keys()))\n    df.to_csv(name, index = False)","execution_count":null,"outputs":[]},{"metadata":{"id":"KOAu_s-1CUCr"},"cell_type":"markdown","source":"## 5.1 Model without any feature selection"},{"metadata":{"trusted":true,"id":"94P5SOkZCUCr","outputId":"8628f12b-97c5-4a78-cc9a-62bc6ca3d4e6"},"cell_type":"code","source":"#We predict the results using the normal trained model\n\nmodel = train_random_forest(train_df)\ny_pred = get_predictions(model)\ncreate_csv(y_pred, name = \"results.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"fgGrd29kCUCs"},"cell_type":"markdown","source":"### Results without any feature selection : 0.897"},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Model with feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction with feature importance technique\n\n#train model\nX_train = np.array(train_fi11_df)\ny_train = np.array(train_df[\"player_profile\"])\n\nmodel = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n\nmodel = model.fit(X_train, y_train)\n\n#predict on trained model\nX_test = np.array(test_fi11_df)\ny_pred = model.predict(X_test)\n\ncreate_csv(y_pred, name = \"featureImportance.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Resuts with feature importance : 0.885 (keeping 19 features), 0.841 (keeping 11 features)"},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Model with univariate selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction with univariate selection technique\n\n#train model\nX_train = np.array(train_univariate18_df)\ny_train = np.array(train_df[\"player_profile\"])\n\nmodel = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n\nmodel = model.fit(X_train, y_train)\n\n#predict on trained model\nX_test = np.array(test_univariate18_df)\ny_pred = model.predict(X_test)\n\ncreate_csv(y_pred, name = \"univariateSelection.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results with univariate selection : 0.879 (keeping 18 features), 0.829 (keeping 13 features)"},{"metadata":{"id":"4JKNxYnDCUCs"},"cell_type":"markdown","source":"## 5.3 Model with feature selection using variance threshold"},{"metadata":{"trusted":true,"id":"7q-4gpFgCUCs"},"cell_type":"code","source":"#Prediction with variance threshold\n\n#train model\nX_train = np.array(train_var_df)\ny_train = np.array(train_df[\"player_profile\"])\n\nmodel = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n\nmodel = model.fit(X_train, y_train)\n\n#predict on trained model\nX_test = np.array(test_var_df)\ny_test = np.array(test_df[\"player_profile\"])\ny_pred = model.predict(X_test)\n\ncreate_csv(y_pred, name = \"resultsVariance.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"vMUAJxu0CUCs"},"cell_type":"markdown","source":"### Result with variance threshold : 0.868 (k = 5)"},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Model with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing train and test data\nscaler = StandardScaler() \n\nX_train = train_df.drop(columns=['player_profile'])\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\ny_train = np.array(train_df[\"player_profile\"])\n\nX_test = test_df.drop(columns=['player_profile'])\nX_test = scaler.transform(X_test)\n\n#PCA\npca = PCA(n_components=20) #20 components = 90% de la variance\npca.fit(X_train)\n\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\n\n#the Random Forest model \nmodel = RandomForestClassifier(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n\nmodel = model.fit(X_train, y_train)\n\n#Prediction\ny_pred = model.predict(X_test)\n\ncreate_csv(y_pred, name = \"pca20.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results with PCA : 0.879 (with 20 new features), 0.868 (13 new features)"},{"metadata":{},"cell_type":"markdown","source":"## 5.5 Conclusion"},{"metadata":{},"cell_type":"markdown","source":"|With new features|With duplication|Dim.Reduction    | Features' number                                  | RESULT            |\n|-----|-----|-----------------------|---------------------------------------------------------|------------|\n|No|No|No|36|0.897|\n|Yes|No|No|62|0.897|\n|No|Yes|No|36|0.897|\n|No|Yes|Feature importance|19|0.885|\n|No|Yes|Feature importance|11|0.841|\n|No|Yes|Univariate selection |18|0.879|\n|No|Yes|Univariate selection |13|0.829|\n|No|Yes|Low variance|16|0.868|\n|No|Yes|Low variance|?|?|\n|No|Yes|PCA|20 new|0.879|\n|No|Yes|PCA|13 new|0.868|"},{"metadata":{},"cell_type":"markdown","source":"Here’s a summary of the results of all the models we’ve tested.\n\n* 1st line: Our initial model without any dimensionality reduction has a result of 0.897\n* 2nd line: By adding features we don’t have a better result so for the others tries we didn’t keep it since we want the reduce the dimension, so it wouldn’t makes sense.\n* 3rd line: By duplicating data we have a similar result too.\n\nAbout dimensionality reduction: \n* For the feature selection, we tested each method two times, keeping more or less features. We see that when we keep around 17-20 features, according the method, it reduces our results by only 0.02, and that dividing almost by two our number of features which is a really interesting result. \n* For feature extraction, with the PCA method, even taking a third of our features’ number, we still have good results.\n\nFinally, if we had to keep only one model in this project we would anyway take the one without any dimensionality reduction because we think that we don’t really have a problem of time compilation or memory space in our case, and we’re not in a situation of over fitting. So we don’t really have an interest in reducing the dimension.\nBut we showed, and that was our objective, that it’s possible to gain lots of benefits by reducing dimension in others situations with a really big amount of features. (And of course there’re a lot of others methods to do that, we've here presented only 4 possibilities)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}